# SEAL Safety System

## Overview

The SEAL API uses a simple LLM-based safety system to ensure all AI-generated content is appropriate for children and educational use.

## How It Works

1. Content is generated by the AI
2. Safety validator checks if content is appropriate for children
3. Safe content is delivered, unsafe content is blocked

## Configuration

Two simple configurations:

```python
# Production (default)
DEFAULT_SAFETY_CONFIG = SafetyConfig(
    enabled=True,
    include_violation_details=False
)

# Development (shows details)
DEV_SAFETY_CONFIG = SafetyConfig(
    enabled=True,
    include_violation_details=True
)
```

Set `ENVIRONMENT=development` to see violation details.

## Safety Criteria

**Safe Content:**
- Positive, encouraging language
- Age-appropriate for children
- Educational value
- Inclusive and respectful

**Unsafe Content:**
- Harmful or negative language
- Inappropriate content
- Discriminatory language
- Unsafe activities

## Usage

```python
from app.safety import LLMSafetyValidator

validator = LLMSafetyValidator()
is_safe, violation = validator.validate_content(content)

if not is_safe:
    print(f"Content blocked: {violation.message}")
```

## Key Files

- `app/safety/config.py` - Configuration
- `app/safety/guardrails.py` - Safety validator
- `app/services/response_processor.py` - API integration

---

**The safety of children is our top priority.** 